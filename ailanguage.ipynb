{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_key = \"8dcXJ41LuAG6wTmqYLWoOAvyDH4JSwHowSBU6VICBhgWUU0LAmrdJQQJ99BFACi5YpzXJ3w3AAAaACOG3smr\"\n",
    "language_endpoint = \"https://1sesac035-language.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.66 \tLength: \t 4 \tOffset: \t 18 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t City \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 7 \tOffset: \t 26 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 1.0 \tLength: \t 9 \tOffset: \t 34 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "# Example function for recognizing entities from text\n",
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"I had a wonderful trip to Seattle last week.\"]\n",
    "        result = client.recognize_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Named Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emotion recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: negative\n",
      "Overall scores: positive=0.00; neutral=0.00; negative=1.00 \n",
      "\n",
      "Sentence: The food and service were unacceptable. \n",
      "Sentence sentiment: negative\n",
      "Sentence score:\n",
      "Positive=0.00\n",
      "Neutral=0.00\n",
      "Negative=1.00\n",
      "\n",
      "......'negative' target 'food'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' target 'service'\n",
      "......Target score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' assessment 'unacceptable'\n",
      "......Assessment score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "\n",
      "\n",
      "Sentence: The concierge was nice, however.\n",
      "Sentence sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.22\n",
      "Neutral=0.75\n",
      "Negative=0.04\n",
      "\n",
      "......'positive' target 'concierge'\n",
      "......Target score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' assessment 'nice'\n",
      "......Assessment score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"The food and service were unacceptable. The concierge was nice, however.\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                print(\"......'{}' target '{}'\".format(target.sentiment, target.text))\n",
    "                print(\"......Target score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    target.confidence_scores.positive,\n",
    "                    target.confidence_scores.negative,\n",
    "                ))\n",
    "                for assessment in mined_opinion.assessments:\n",
    "                    print(\"......'{}' assessment '{}'\".format(assessment.sentiment, assessment.text))\n",
    "                    print(\"......Assessment score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        assessment.confidence_scores.positive,\n",
    "                        assessment.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  French\n"
     ]
    }
   ],
   "source": [
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"Ce document est rédigé en Français.\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. This feature is provided as an API for developers. Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations.\n"
     ]
    }
   ],
   "source": [
    "def sample_extractive_summarization(client):\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction\n",
    "    ) \n",
    "\n",
    "    document = [\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. \"\n",
    "        \"These sentences collectively convey the main idea of the document. This feature is provided as an API for developers. \" \n",
    "        \"They can use it to build intelligent solutions based on the relevant information extracted to support various use cases. \"\n",
    "        \"Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations. \"\n",
    "        \"It draws its strength from transfer learning across monolingual and harness the shared nature of languages to produce models of improved quality and efficiency. \"\n",
    "    ]\n",
    "\n",
    "    poller = client.begin_analyze_actions(\n",
    "        document,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=4)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for result in document_results:\n",
    "        extract_summary_result = result[0]  # first document, first result\n",
    "        if extract_summary_result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                extract_summary_result.code, extract_summary_result.message\n",
    "            ))\n",
    "        else:\n",
    "            print(\"Summary extracted: \\n{}\".format(\n",
    "                \" \".join([sentence.text for sentence in extract_summary_result.sentences]))\n",
    "            )\n",
    "\n",
    "sample_extractive_summarization(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abstract summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries abstracted:\n",
      "The Chief Technology Officer of Azure AI Cognitive Services discusses Microsoft's commitment to advancing AI by integrating monolingual text, audio or visual signals, and multilingual capabilities into a unified approach, termed XYZ-code. This innovative framework aims to create AI that can better understand and communicate across various modalities and languages, akin to human cognition. Through their efforts, Microsoft has achieved human-level performance in several key AI domains, including speech recognition, machine translation, and image captioning. The ultimate goal is to develop pretrained models that can learn from multiple sources and transfer knowledge across tasks, emulating the breadth of human learning. The XYZ-code is seen as a critical element in realizing this vision, with the potential to revolutionize AI's ability to process and understand diverse types of information. This approach not only demonstrates current progress but also sets a direction for future breakthroughs in multisensory and multilingual AI learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_abstractive_summarization() -> None:\n",
    "    \n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    document = [\n",
    "        \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
    "        \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
    "        \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
    "        \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
    "        \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
    "        \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
    "        \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
    "        \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
    "        \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
    "        \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
    "        \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
    "        \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
    "        \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
    "        \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
    "        \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
    "        \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "    ]\n",
    "\n",
    "    poller = client.begin_abstract_summary(document)\n",
    "    abstract_summary_results = poller.result()\n",
    "    for result in abstract_summary_results:\n",
    "        if result.kind == \"AbstractiveSummarization\":\n",
    "            print(\"Summaries abstracted:\")\n",
    "            [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "        elif result.is_error is True:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_abstractive_summarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -X POST \"https://1sesac035-language.cognitiveservices.azure.com/language/:query-knowledgebases?projectName=QnAProj&api-version=2021-10-01&deploymentName=production\" -H \"Ocp-Apim-Subscription-Key: 8dcXJ41LuAG6wTmqYLWoOAvyDH4JSwHowSBU6VICBhgWUU0LAmrdJQQJ99BFACi5YpzXJ3w3AAAaACOG3smr\" -H \"Content-Type: application/json\" -d \"{\\\"top\\\":3,\\\"question\\\":\\\"YOUR_QUESTION_HERE\\\",\\\"includeUnstructuredSources\\\":true,\\\"confidenceScoreThreshold\\\":\\\"YOUR_SCORE_THRESHOLD_HERE\\\",\\\"answerSpanRequest\\\":{\\\"enable\\\":true,\\\"topAnswersWithSpan\\\":1,\\\"confidenceScoreThreshold\\\":\\\"YOUR_SCORE_THRESHOLD_HERE\\\"},\\\"filters\\\":{\\\"metadataFilter\\\":{\\\"logicalOperation\\\":\\\"YOUR_LOGICAL_OPERATION_HERE\\\",\\\"metadata\\\":[{\\\"key\\\":\\\"YOUR_ADDITIONAL_PROP_KEY_HERE\\\",\\\"value\\\":\\\"YOUR_ADDITIONAL_PROP_VALUE_HERE\\\"}]}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 비용은 얼마인가요?\n",
      "A: Microsoft Learn 교육은 완전히 무료이며 Microsoft 제품에 관심이 있는 사람이라면 누구나 사용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.questionanswering import QuestionAnsweringClient\n",
    "\n",
    "endpoint = \"https://1sesac035-language.cognitiveservices.azure.com/\"\n",
    "credential = AzureKeyCredential(\"8dcXJ41LuAG6wTmqYLWoOAvyDH4JSwHowSBU6VICBhgWUU0LAmrdJQQJ99BFACi5YpzXJ3w3AAAaACOG3smr\")\n",
    "knowledge_base_project = \"QnAProj\"\n",
    "deployment = \"production\"\n",
    "\n",
    "def main():\n",
    "    client = QuestionAnsweringClient(endpoint, credential)\n",
    "    with client:\n",
    "        question=\"비용은 얼마인가요?\"\n",
    "        output = client.get_answers(\n",
    "            question = question,\n",
    "            project_name=knowledge_base_project,\n",
    "            deployment_name=deployment\n",
    "        )\n",
    "    print(\"Q: {}\".format(question))\n",
    "    print(\"A: {}\".format(output.answers[0].answer))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### microsoft azure translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J’aimerais vraiment conduire votre voiture autour du pâté de maisons plusieurs fois !\n",
      "Ngingathanda ukushayela imoto yakho ezungeze ibhlokhi izikhathi ezimbalwa!\n",
      "나는 정말로 당신의 차를 몇 번 블록 주위에 운전하고 싶습니다!\n"
     ]
    }
   ],
   "source": [
    "import requests, uuid, json\n",
    "\n",
    "# Add your key and endpoint\n",
    "key = \"5BOOH90Qyj7T9opgcLQs1dFZWcfHhMRvLiqOMlHPhJM4TaJCjHEAJQQJ99BFACi5YpzXJ3w3AAAbACOGt27F\"\n",
    "endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "\n",
    "# location, also known as region.\n",
    "# required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
    "location = \"northeurope\"\n",
    "\n",
    "path = '/translate'\n",
    "constructed_url = endpoint + path\n",
    "\n",
    "params = {\n",
    "    'api-version': '3.0',\n",
    "    'from': 'en',\n",
    "    'to': ['fr', 'zu', 'ko']\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': key,\n",
    "    # location required if you're using a multi-service or regional (not global) resource.\n",
    "    'Ocp-Apim-Subscription-Region': location,\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())\n",
    "}\n",
    "\n",
    "# You can pass more than one object in body.\n",
    "body = [{\n",
    "    'text': 'I would really like to drive your car around the block a few times!'\n",
    "}]\n",
    "\n",
    "request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "response = request.json()\n",
    "\n",
    "# text만 추출\n",
    "texts = [t['text'] for t in response[0]['translations']]\n",
    "\n",
    "# 출력\n",
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi language translator in gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def translate_with_new_function(input_text):\n",
    "    # 새 변수 이름 (기존과 충돌 방지)\n",
    "    subscription_key = \"5BOOH90Qyj7T9opgcLQs1dFZWcfHhMRvLiqOMlHPhJM4TaJCjHEAJQQJ99BFACi5YpzXJ3w3AAAbACOGt27F\"\n",
    "    service_endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "    service_region = \"northeurope\"\n",
    "\n",
    "    api_path = '/translate'\n",
    "    url = service_endpoint + api_path\n",
    "\n",
    "    query_params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'en',\n",
    "        'to': ['fr', 'zu', 'ko']\n",
    "    }\n",
    "\n",
    "    request_headers = {\n",
    "        'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "        'Ocp-Apim-Subscription-Region': service_region,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    request_body = [{'text': input_text}]\n",
    "\n",
    "    response_obj = requests.post(url, params=query_params, headers=request_headers, json=request_body)\n",
    "    response_json = response_obj.json()\n",
    "\n",
    "    translated_texts = [item['text'] for item in response_json[0]['translations']]\n",
    "    return translated_texts\n",
    "\n",
    "gr.Interface(\n",
    "    fn=translate_with_new_function,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Multi-language Translator\"\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
